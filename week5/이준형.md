## 9장 웹 로봇 (p.247)
- 웹 로봇은 사람과의 상호작용 없이 연속된 웹 트랜잭션들을 자동으로 수행하는 소프트웨어 프로그램
- 마치 사람처럼 하이퍼링크를 따라가면서 컨텐츠를 수집함
- 방식에 따라 크롤러, 스파이더, 웜, 봇 등으로 각양각색의 이름으로 불림
- 예를 들어.
  - 주식 차트 데이터 수집
  - 웹 통계 조사
  - 검색엔진
  - 가격 데이터 수집하여 가격 비교

### 9.1 크롤러와 크롤링(p.248)
- 웹 크롤러는 먼저 웹 페이지를 한 개 가져오고, 그 페이지 내에 있는 링크들을 재귀적으로 순회한다.
- 이러한 동작이 웹을 따라 기어다니는 것과 같다고 하여 crawler 라는 이름이 붙었다.

#### 9.1.1 어디에서 시작하는가: '루트 집합'(p.248)
- 크롤링의 시작점이 되는 최초의 문서 집합을 루트 집합이라고 한다.
- 루트 집합을 잘 선택해야 최대한 많은 웹 문서에 접근할 수 있음.
- 일반적으로 좋은 루트 집합은 크고 인기 있는 웹 사이트이다.

#### 9.1.2 링크 추출과 상대 링크 정상화(p.249)
- 크롤러는 수집된 링크를 상대링크에서 절대링크로 변환할 필요가 있다.
- 자세한 내용은 뒤에서 다시 이야기한다고 함.

#### 9.1.3 순환 피하기(p.249)
- 링크를 타고 다니기 때문에 순환을 조심해야한다.
- 순환을 피하기 위해서는 어디에 방문했는지 저장해야한다.

#### 9.1.4 루프와 중복(p.250)
- 순환은 다음과 같은 3가지 이유로 크롤러에게 해로움.
- 첫째, 루프에 빠져 같은 페이지를 돌면서 모든 시간, 대역폭을 낭비하게 만들 수 있다.
- 둘째, 크롤러가 같은 페이지를 계속 가져오면 이는 고스란히 웹 서버의 부담이 된다.(DoS)
- 셋째, 루프를 돌며 동일한 컨텐츠를 계속 수집하게 되어 중복 컨텐츠로 넘쳐나게 된다.

#### 9.1.5 빵 부스러기의 흔적(p.251)
- 불행히도, 방문한 곳을 지속적으로 추적하는게 쉽지 않음.
- 방문했던 URL을 저장하기 위한 자료구조는 속도와 메모리 사용 면에서 효과적이어야 한다.
- 검색 트리나 해시 테이블이 필요할 것임.
- 복잡한 로봇이라면 **검색 트리 or 해시 테이블 사용**
- 공간 사용을 최소화 하기 위해서는 **존재 비트 배열(presence bit array)와 같은 느슨한 자료 구조**를 사용 -> 각 URL은 해시 함수에 의해 고정된 크기의 숫자로 변환
- 갑자기 중단될 경우를 대비해 **체크포인트** 저장
- 웹이 성장하면서 한 대의 컴퓨터에서 하나의 로봇이 크롤링을 완수하는 것은 불가능해졌음. -> 여러 로봇이 데이터를 주고 받으며 동시에 크롤링

#### 9.1.6 별칭(alias)과 로봇 순환(p.252)
- 올바른 자료 구조를 갖추었더라도 URL이 별칭을 가질 수 있는 이상 어떤 페이지를 방문했었는지 말해주는게 쉽지 않을 때도 있음.
- [표 9-1]에 나온것처럼 동일 리소스에 대해 서로 다른 URL이 존재한다.

#### 9.1.7 URL 정규화하기(p.253)
- 앞서 같은 리소스에 대해 서로 다른 URL이 존재하는 것을 정규화 함으로써 중복을 제거하려고 노력한다.
- 포트 번호 명시, 이스케이핑 제거, # 태그 제거 등

#### 9.1.8 파일 시스템 링크 순환(p.254)
- 아마 디렉토리 리스팅이 되는 경우 심볼링 링크(바로가기)로 인해 발생하는 순환을 이야기 하는 듯 하다.
- 해결 방법은 안나와있음.

#### 9.1.9 동적 가상 웹 공간(p.255)
- 악의적인 웹 페이지 관리자가 복잡한 크롤러 루프를 만들 수도 있다.
- 일반 URL로 보이는 URL에 CGI로 동적으로 HTML 페이지를 생성해서 새로운 웹 페이지를 계속 만들어낼 수도 있다.
- 악의적인 뜻이 없음에도, 자신도 모르게 심볼릭 링크나 동적 콘텐츠로 인해 루프를 만들어낼 수 있다.

#### 9.1.10 루프와 중복 피하기(p.256)
- 모든 순환을 피하는 완벽한 방법은 없다. 휴리스틱을 활용하여 회피한다.
- 휴리스틱은 문제를 피하는데 도움은 주지만 약간 '손실'을 유발할 수도 있다.
- 의심스러워 보이지만 실은 유효한 콘텐츠를 걸러버릴 수도 있기 때문
- 다음과 같은 방법들이 사용된다.
    - URL 정규화 : 앞서 나옴. 실제로 같은 리소스지만 서로 달라보이는 URL을 맞춰준다.
    - 너비 우선 크롤링 : 깊이가 아니라 너비 우선으로 탐색하고, 최대 깊이를 제한한다.
    - 스로틀링 : 일정 시간동안 크롤링 하는 웹 페이지의 숫자를 제한, 특정 버서의 총 접근 횟수 등을 제한
    - URL 크기 제한 : 심볼릭 링크 등의 순환으로 무한히 URL 길이가 길어지는 경우를 제한
    - URL/사이트 블랙리스트 : 악의적으로 보이는 URL이나 사이트를 크롤링하지 않도록 한다.
    - 패턴 발견 : 동일 디렉토리 패턴이 반복되는 것을 감지
    - 콘텐츠 지문 : 콘텐츠의 체크섬 등을 이용해 동일 컨텐츠인지 판단
    - 사람의 모니터링 : 위의 이상 상황을 감지하여 사람이 모니터링 할 수 있도록 만듬
- 좋은 휴리스틱을 만드는 일은 현재 진행형임.

### 9.2 로봇의 HTTP(p.259)
- 로봇도 다른 HTTP 클라이언트처럼 HTTP 명세를 지켜야함.
- 하지만, 많은 로봇들이 그들이 찾는 컨텐츠를 요청하기에 필요한 최소한의 HTTP 만을 구현하려고한다.
- 그래서 많은 로봇이 HTTP/1.0 요청을 보낸다.

#### 9.2.1 요청 헤더 식별하기(p.259)
- 로봇은 User-Agent와 기타 헤더를 통해 로봇의 능력, 신원, 출신을 알린다.
- User-Agent: 로봇의 이름
- From: 로봇의 사용자/관리자의 이메일 주소
- Accept: 원래 의도와 같음
- Referer: 원래 의도와 같음

#### 9.2.2 가상 호스팅(p.260)
- 로봇은 Host 헤더를 지원할 필요가 있다.
- 가상 호스팅 환경에서는 Host 헤더를 포함하지 않을 경우 잘못된 콘텐츠를 찾을 수 있다.
- 이러한 이유로 HTTP/1.1은 Host 해더가 필수이다.
- 대부분의 서버는 Host 헤더가 없을 때 제공하는 디폴트 가상 호스팅이 있기 때문에 잘못된 컨텐츠를 크롤링 할 수도 있다.[그림 9-5] 참고

#### 9.2.3 조건부 요청(p.261)
- 로봇은 매우 많은 양의 문서를 요청하기 때문에 변경 되었을 때만 컨텐츠를 가져오도록 하는 것이 매우 의미가 있다.
- 마치 캐시처럼 동작함

#### 9.2.4 응답 다루기(p.262)
- 로봇들은 단순히 GET 메소드로 콘텐츠를 요청해서 가져오는 것이 관심사다.
- 로봇은 상태코드(200 or 404 등)나 엔터티를 이해해야한다.

#### 9.2.5 User-Agent 타기팅(p.263)
- 웹 서버 관리자는 일반적인 클라이언트 뿐만 아니라, 로봇에 의한 방문도 염두해야한다.
- ex) 서버 사이드 렌더링

### 9.3 부적절하게 동작하는 로봇들(p.263)
- 폭주하는 로봇 : 너무 많은 요청을 날려서 웹 서버에 극심한 부하를 줄 수도 있다. 로봇 관리자는 이런 폭주를 방지하는 보호 장치를 만들어야한다.
- 오래된 URL : 로봇이 예전에 만들어둔 URL 목록을 방문하려고 하는 경우, 해당 URL의 문서들이 삭제되었다면 다량의 에러 로그를 유발할 수 있다.
- 길고 잘못된 URL, 호기심이 지나친 로봇, 동적 게이트웨이 접근 등도 있음.

### 9.4 로봇 차단하기(p.265)
- 정상적인 웹 크롤러는 robots.txt를 통해 어떤 페이지를 크롤링해도 되는지 확인한 후 크롤링한다.

#### 9.4.1 로봇 차단 표준(p.266)
- 로봇 차단 표준에는 v0.0, v1.0, v2.0이 있고, v0.0과 v1.0이 널리 쓰이고 있음.
- 책에서는 v0.0과 호환되는 v1.0에 초점을 맞춰 설명한다.

#### 9.4.2 웹 사이트의 robots.txt 파일들(p.267)
- 기본적으로 사이트 전체에 대한 robots.txt 파일은 단 하나가 존재하지만, 가상 호스팅된 환경이라면 서로 다른 docroot에 서로 다른 robots.txt가 있을 수 있다.
- robots.txt는 루트 디렉토리에 위치해야한다.
- 로봇은 어떤 웹 사이트든 /robots.txt 부터 찾아본다.
- 응답이 200이라면 해당 파일의 로봇 차단 규칙대로 크롤링을 한다.
- 응답이 404했다면 로봇은 제약 없이 해당 사이트를 크롤링한다.
- 401, 403이면 접근 제한 / 503이면 나중으로 미룸 / 3xx라면 리소스가 발견될 때까지 리다이렉트

#### 9.4.3 robots.txt 파일 포맷(p.268)
- 마치 HTTP 헤더처럼 생겼다.
- 예시(https://www.dcinside.com/robots.txt)
```
User-agent: *
Disallow: /

# Ads
User-agent: grapeshot
Allow: /  

# Search
User-agent: Googlebot
Allow : / 
Crawl-delay: 60


User-agent: grapeshot
Allow: /
```
- User-Agent 별로, 크롤링을 허용할 경로와 불허할 경로를 지정한다.

#### 9.4.4 그 외에 알아둘 점(p.271)
- 명세가 발달함에 따라 다른 필드가 추가될 수 있음. 로봇은 자신이 이해 못하는 필드는 무시해야한다.
- 하위 호환성을 위해 한 줄을 여러 줄로 나누어 적는 것은 허용되지 않음.
- 주석은 #으로 해당 줄의 # 뒤의 모든 문자를 주석으로 간주한다.
- v0.0은 Allow 줄을 지원하지 않는다. 때문에 Allow로 허용했더라도, 크롤링하지 않을 수 있다.

#### 9.4.5 robots.txt의 캐싱과 만료(p.271)
- 크롤러도 클라이언트처럼 robots.txt를 캐싱한다.
- 대개의 크롤러는 HTTP/1.0 기준으로 되어있어서 이에 적절한 캐시 지시자를 사용해야한다.

#### 9.4.6 로봇 차단 펄 코드(p.272)
- Perl로도 로봇 차단 코드를 만들 수 있다.
- 코드는 그냥 참고해보고, 여기에 나온 robots.txt와 [표 9-4]를 비교해보면 좋을 것 같다.

#### 9.4.7 HTML 로봇 제어 META 태그(p.274)
- robots.txt의 단점 중 하나는 그 파일을 콘텐츠의 작성자 개개인이 아니라, 웹 사이트 관리자가 소유한다는 것.
- 때문에 HTML 태그로도 크롤러에게 문서를 색인할지 알려줄 수 있다.
```xml
<META NAME="ROBOTS" CONTENT={directive-list}>
위와 같은 식이다.

현재 페이지 ㄴㄴ : <META NAME="ROBOTS" CONTENT="NOINDEX">
현재 페이지에 링크된 페이지 ㄴㄴ : <META NAME="ROBOTS" CONTENT="NOFOLLOW">
```
- 그 외에 directive-list
    - INDEX, FOLLOW, NOARCHIVE, ALL, NONE
- HTML 문서의 HEAD 섹션에 와야한다.
- name, content의 대소문자를 구분하지 않음.
- 이 외에 검색엔진에게 노출될 문구를 설정해줄 수 있다.
```xml
<meta name="description" content="메리의 골동품 상점 웹 사이트에 오신것을 환영합니다.">
<meta name="keywords" content="antiques,mary,furniture,restoration">
<meta name="revisit-after" content="10 days">
```

### 9.5 로봇 에티켓(p.277)
- 1993년에 웹 로봇 커뮤니티의 개척자인 마틴 코스터에 의해 웹 로봇을 만드는 사람들을 위한 가이드라인 목록을 작성했음.
- 해당 가이드라인은 로봇을 대상으로 한 것이지만, 가벼운 크롤러도 마찬가지다.
- 1. 신원식별
    - 로봇의 신원을 밝히라
    - 기계의 신원을 밝히라
    - 연락처를 밝히라
- 2. 동작
    - 긴장하라
    - 대비하라
    - 감시와 로그
    - 배우고 조정하라
- 3. 스스로를 제한하라
    - URL을 필터링하라
    - 동적 URL을 필터링하라
    - Accept 관련 헤더로 필터링
    - robots.txt에 따르라
    - 스스로를 억제하라
- 4. 루프와 중복을 견뎌내기, 그리고 그 외의 문제들
    - 모든 응답 코드 다루기
    - URL 정규화하기
    - 적극적으로 순환 피하기
    - 함정을 감시하라
    - 블랙리스트를 관리하라
- 5. 확장성
    - 공간 이해하기
    - 대역폭 이해하기
    - 시간 이해하기
    - 분할 정복
- 6. 신뢰성
    - 철저하게 테스트하라
    - 체크포인트
    - 실패에 대한 유연성
- 7. 소통
    - 준비하라
    - 이해하라
    - 즉각 대응하라

### 9.6 검색엔진(p.280)
- 웹 로봇을 가장 광범위하게 사용하는 것은 인터넷 검색 엔진이다.
- 웹 크롤러는 마치 먹이를 주듯 검색엔진에게 웹에 존재하는 문서들을 가져다 준다.

#### 9.6.1 넓게 생각하라(p.280)
- 십억개의 문서를 하나당 0.5초씩 크롤링 한다고 하면 5,700일이나 걸린다.
- 당연히 이를 수행하려면 병렬적으로 처리해야한다. 하지만 그렇다고 하더라도 웹 전체를 크롤링하는 것은 여전히 쉽지 않은 도전이다.

#### 9.6.2 현대적인 검색엔진의 아키텍처(p.281)
- 오늘날 검색엔진들은 그들이 가지고 있는 전 세계의 웹페이지들에 대해 '풀 텍스트 색인'을 통해 검색 기능을 제공한다.

#### 9.6.3 풀 텍스트 색인(p.281)
- 풀 텍스트 색인은 단어 하나를 입력 받아 그 단어를 포함하고 있는 문서를 즉각 알려줄 수 있는 데이터베이스이다. (Like ES)
- [그림 9-8] 참고

#### 9.6.4 질의 보내기(p.282)
- 일반적으로 GET 요청으로 query parameter에 검색어를 요청하면 응답으로 검색 결과 페이지를 보여준다.

#### 9.6.5 검색 결과를 정렬하고 보여주기(p.283)
- 한번 질의를 했던 내용이라면 그대로 결과 페이지를 만들어낼 수 있다.
- 많은 웹 페이지가 질의로 들어온 단어를 사용할 수 있기 때문에 검색 엔진은 똑똑한 알고리즘을 통해 문서들 사이에 순위를 매긴다.
- 이를 관련도 랭킹 이라고 한다.

#### 9.6.6 스푸핑(p.284)
- 웹 마스터는 사이트가 검색 노출 상단에 노축되도록 하고 싶다.
- 때문에 웹 마스터들은 고의로 검색 노출 상단에 노출되기 위해 갖가지 방법을 고안해냈고, 반대로 검색엔진과 크롤러는 이를 잘 잡아내기 위해 노력한다.


## 10장 HTTP/2.0(p.287)
- 