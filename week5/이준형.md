## 9장 웹 로봇 (p.247)
- 웹 로봇은 사람과의 상호작용 없이 연속된 웹 트랜잭션들을 자동으로 수행하는 소프트웨어 프로그램
- 마치 사람처럼 하이퍼링크를 따라가면서 컨텐츠를 수집함
- 방식에 따라 크롤러, 스파이더, 웜, 봇 등으로 각양각색의 이름으로 불림
- 예를 들어.
  - 주식 차트 데이터 수집
  - 웹 통계 조사
  - 검색엔진
  - 가격 데이터 수집하여 가격 비교

### 9.1 크롤러와 크롤링(p.248)
- 웹 크롤러는 먼저 웹 페이지를 한 개 가져오고, 그 페이지 내에 있는 링크들을 재귀적으로 순회한다.
- 이러한 동작이 웹을 따라 기어다니는 것과 같다고 하여 crawler 라는 이름이 붙었다.

#### 9.1.1 어디에서 시작하는가: '루트 집합'(p.248)
- 크롤링의 시작점이 되는 최초의 문서 집합을 루트 집합이라고 한다.
- 루트 집합을 잘 선택해야 최대한 많은 웹 문서에 접근할 수 있음.
- 일반적으로 좋은 루트 집합은 크고 인기 있는 웹 사이트이다.

#### 9.1.2 링크 추출과 상대 링크 정상화(p.249)
- 크롤러는 수집된 링크를 상대링크에서 절대링크로 변환할 필요가 있다.
- 자세한 내용은 뒤에서 다시 이야기한다고 함.

#### 9.1.3 순환 피하기(p.249)
- 링크를 타고 다니기 때문에 순환을 조심해야한다.
- 순환을 피하기 위해서는 어디에 방문했는지 저장해야한다.

#### 9.1.4 루프와 중복(p.250)
- 순환은 다음과 같은 3가지 이유로 크롤러에게 해로움.
- 첫째, 루프에 빠져 같은 페이지를 돌면서 모든 시간, 대역폭을 낭비하게 만들 수 있다.
- 둘째, 크롤러가 같은 페이지를 계속 가져오면 이는 고스란히 웹 서버의 부담이 된다.(DoS)
- 셋째, 루프를 돌며 동일한 컨텐츠를 계속 수집하게 되어 중복 컨텐츠로 넘쳐나게 된다.

#### 9.1.5 빵 부스러기의 흔적(p.251)
- 불행히도, 방문한 곳을 지속적으로 추적하는게 쉽지 않음.
- 방문했던 URL을 저장하기 위한 자료구조는 속도와 메모리 사용 면에서 효과적이어야 한다.
- 검색 트리나 해시 테이블이 필요할 것임.
- 복잡한 로봇이라면 **검색 트리 or 해시 테이블 사용**
- 공간 사용을 최소화 하기 위해서는 **존재 비트 배열(presence bit array)와 같은 느슨한 자료 구조**를 사용 -> 각 URL은 해시 함수에 의해 고정된 크기의 숫자로 변환
- 갑자기 중단될 경우를 대비해 **체크포인트** 저장
- 웹이 성장하면서 한 대의 컴퓨터에서 하나의 로봇이 크롤링을 완수하는 것은 불가능해졌음. -> 여러 로봇이 데이터를 주고 받으며 동시에 크롤링

#### 9.1.6 별칭(alias)과 로봇 순환(p.252)
- 올바른 자료 구조를 갖추었더라도 URL이 별칭을 가질 수 있는 이상 어떤 페이지를 방문했었는지 말해주는게 쉽지 않을 때도 있음.
- [표 9-1]에 나온것처럼 동일 리소스에 대해 서로 다른 URL이 존재한다.

#### 9.1.7 URL 정규화하기(p.253)
- 앞서 같은 리소스에 대해 서로 다른 URL이 존재하는 것을 정규화 함으로써 중복을 제거하려고 노력한다.
- 포트 번호 명시, 이스케이핑 제거, # 태그 제거 등

#### 9.1.8 파일 시스템 링크 순환(p.254)
- 아마 디렉토리 리스팅이 되는 경우 심볼링 링크(바로가기)로 인해 발생하는 순환을 이야기 하는 듯 하다.
- 해결 방법은 안나와있음.

#### 9.1.9 동적 가상 웹 공간(p.255)
- 악의적인 웹 페이지 관리자가 복잡한 크롤러 루프를 만들 수도 있다.
- 일반 URL로 보이는 URL에 CGI로 동적으로 HTML 페이지를 생성해서 새로운 웹 페이지를 계속 만들어낼 수도 있다.
- 악의적인 뜻이 없음에도, 자신도 모르게 심볼릭 링크나 동적 콘텐츠로 인해 루프를 만들어낼 수 있다.

#### 9.1.10 루프와 중복 피하기(p.256)
- 모든 순환을 피하는 완벽한 방법은 없다. 휴리스틱을 활용하여 회피한다.
- 휴리스틱은 문제를 피하는데 도움은 주지만 약간 '손실'을 유발할 수도 있다.
- 의심스러워 보이지만 실은 유효한 콘텐츠를 걸러버릴 수도 있기 때문
- 다음과 같은 방법들이 사용된다.
    - URL 정규화 : 앞서 나옴. 실제로 같은 리소스지만 서로 달라보이는 URL을 맞춰준다.
    - 너비 우선 크롤링 : 깊이가 아니라 너비 우선으로 탐색하고, 최대 깊이를 제한한다.
    - 스로틀링 : 일정 시간동안 크롤링 하는 웹 페이지의 숫자를 제한, 특정 버서의 총 접근 횟수 등을 제한
    - URL 크기 제한 : 심볼릭 링크 등의 순환으로 무한히 URL 길이가 길어지는 경우를 제한
    - URL/사이트 블랙리스트 : 악의적으로 보이는 URL이나 사이트를 크롤링하지 않도록 한다.
    - 패턴 발견 : 동일 디렉토리 패턴이 반복되는 것을 감지
    - 콘텐츠 지문 : 콘텐츠의 체크섬 등을 이용해 동일 컨텐츠인지 판단
    - 사람의 모니터링 : 위의 이상 상황을 감지하여 사람이 모니터링 할 수 있도록 만듬
- 좋은 휴리스틱을 만드는 일은 현재 진행형임.

### 9.2 로봇의 HTTP(p.259)